{
    "additionalProperties": false,
    "error_msg": "ViT parameters must be one(s) of ['epochs', 'batch size', 'weight decay', 'learning_rate', 'device'].",
    "description": "ViT is a transformer that allows you to classify text in English.",
    "properties": {
        "epochs": {
            "oneOf": [
                {
                    "error_msg": "A whole number greater than or equal to 1 must be entered.",
                    "description": "Total number of training epochs to perform.",
                    "type": "integer",
                    "minimum": 1,
                    "default": 3
                }
            ]
        },
        "batch_size": {
            "oneOf": [
                {
                    "error_msg": "A whole number greater than or equal to 1 must be entered.",
                    "description": "The batch size per GPU/TPU core/CPU for training",
                    "type": "integer",
                    "minimum": 1,
                    "default": 8
                }
            ]
        },
        "learning_rate": {
            "oneOf": [
                {
                    "error_msg": "Must be a positive number. A number between 10e-6 and 1 is recommended.",
                    "description": "The initial learning rate for AdamW optimizer",
                    "type": "number",
                    "minimum": 0,
                    "default": 5e-5
                }
            ]
        },
        "device": {
            "oneOf": [
                {
                    "error_msg": "The parameter must be in string format and can be either 'gpu' or 'cpu'.",
                    "description": "Hardware on which the training is run. If available, GPU is recommended for efficiency reasons. Otherwise, use CPU.",
                    "type": "string",
                    "default": "gpu",
                    "enum": ["gpu", "cpu"]
                }
            ]
        },
        "weight_decay": {
            "oneOf": [
                {
                    "error_msg": "A number between 0 and 1 must be entered.",
                    "description": "Weight decay is a regularization technique used in training neural networks to prevent overfitting. In the context of the AdamW optimizer, the 'weight_decay' parameter is the rate at which the weights of all layers are reduced during training, provided that this rate is not zero.",
                    "type": "number",
                    "minimum": 0,
                    "default": 0
                }
            ]
        }
    },
    "type": "object"
}
